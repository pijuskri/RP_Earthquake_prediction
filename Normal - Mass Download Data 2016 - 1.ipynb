{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aef4291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpl_toolkits\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import logging\n",
    "from obspy import read\n",
    "import time\n",
    "\n",
    "\n",
    "from obspy.clients.fdsn.mass_downloader import RectangularDomain, Restrictions, MassDownloader\n",
    "from scipy import signal\n",
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client as FDSN_Client\n",
    "from obspy import read_inventory\n",
    "import asyncio\n",
    "from itertools import islice\n",
    "from itertools import tee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7db0d9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88740, 7)\n"
     ]
    }
   ],
   "source": [
    "events_df = pd.read_pickle('data/events_processed.pkl')\n",
    "#events_df = pd.read_pickle('data/events_deep_processed.pkl')\n",
    "stations_df = pd.read_pickle('data/stations_processed.pkl')\n",
    "\n",
    "selected_stations = ['BFZ', 'BKZ', 'DCZ', 'DSZ', 'HIZ', 'JCZ', 'KHZ', 'KUZ', 'LBZ',\n",
    "                        'MSZ', 'MWZ', 'MXZ', 'NNZ', 'ODZ', 'OPRZ', 'OUZ', 'PUZ', 'PXZ', 'QRZ', 'RPZ',\n",
    "                         'SYZ', 'THZ', 'TOZ', 'URZ', 'VRZ', 'WHZ', 'WIZ', 'WKZ', 'WVZ']\n",
    "\n",
    "#events_full = events_df[(events_df.time > '2016-01-01') & (events_df.time < '2017-01-01')]\n",
    "events_full = events_df\n",
    "print(events_full.shape)\n",
    "folder = \"active\" # normal/active\n",
    "single = False # should download only closest?\n",
    "station_to_get = None#selected_stations[0]\n",
    "all_station_string = \",\".join([station.station_code for j, station in stations_df.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cdb2bf4-417e-4e7c-8579-75d2d13f5733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events = events_full#[0:3000]\n",
    "stations_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0cba9d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-06-02 16:28:08,116] - obspy.clients.fdsn.mass_downloader - INFO: Initializing FDSN client(s) for GEONET.\n",
      "[2022-06-02 16:28:10,462] - obspy.clients.fdsn.mass_downloader - INFO: Successfully initialized 1 client(s): GEONET.\n"
     ]
    }
   ],
   "source": [
    "mdl = MassDownloader(providers=['GEONET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef9b2922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mass_data_downloader(start, stop, event_id, Station,\n",
    "                         Network='NZ', \n",
    "                         Channel='HHZ', \n",
    "                         Location=10,\n",
    "                         folder='normal'\n",
    "                         ):\n",
    "    '''\n",
    "    This function uses the FDSN mass data downloader to automatically download\n",
    "    data from the XH network deployed on the RIS from Nov 2014 - Nov 2016.\n",
    "    More information on the Obspy mass downloader available at:\n",
    "    https://docs.obspy.org/packages/autogen/obspy.clients.fdsn.mass_downloader.html\n",
    "    Inputs:\n",
    "    start: \"YYYYMMDD\"\n",
    "    stop:  \"YYYYMMDD\"\n",
    "    Network: 2-character FDSN network code\n",
    "    Station: 2-character station code\n",
    "    Channel: 3-character channel code\n",
    "    Location: 10.\n",
    "    '''\n",
    "    #print(\"=\" * 65)\n",
    "    #print(\"Initiating mass download request.\")\n",
    "    #print('downloading: ',event_id)\n",
    "\n",
    "    domain = RectangularDomain(\n",
    "        minlatitude=-47.749,\n",
    "        maxlatitude=-33.779,\n",
    "        minlongitude=166.104,\n",
    "        maxlongitude=178.990\n",
    "    )\n",
    "\n",
    "    restrictions = Restrictions(\n",
    "        starttime = start,\n",
    "        endtime = stop,\n",
    "        chunklength_in_sec = None,\n",
    "        network = Network,\n",
    "        station = Station,\n",
    "        location = Location,\n",
    "        channel = Channel,\n",
    "        reject_channels_with_gaps = False,\n",
    "        minimum_length = 0.0,\n",
    "        minimum_interstation_distance_in_m = 100.0\n",
    "    )\n",
    "\n",
    "    #mdl = MassDownloader(providers=['GEONET'])\n",
    "    ev_str = str(event_id).replace(\":\", \"_\")\n",
    "    try:\n",
    "        mdl.download(\n",
    "            domain,\n",
    "            restrictions,\n",
    "            mseed_storage=f\"datasets/{folder}/waveforms/{ev_str}\",\n",
    "            stationxml_storage=f\"datasets/{folder}/stations\",\n",
    "        )\n",
    "    except:\n",
    "        pass\n",
    "    #print('done: ',event_id)\n",
    "\n",
    "logger = logging.getLogger(\"obspy.clients.fdsn.mass_downloader\")\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae9a12ed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#async def final_download():\n",
    "#    for i, event in events.iterrows():\n",
    "#        event_id = event.event_id\n",
    "#        event_time = event['time']  \n",
    "#        start=event_time - T_event\n",
    "#        end=event_time + H_event\n",
    "#        \n",
    "#        print(\"=\" * 65)\n",
    "#        print(\"Initiating mass download request.\")\n",
    "#        print(event_id)\n",
    "#        #asyncio.run(main())\n",
    "#        #tasks = [download_st(start, end, event_id, station) for j, station in stations_df.iterrows()]\n",
    "#        \n",
    "#        #tasks = [asyncio.to_thread(mass_data_downloader, start, end, event_id, station.station_code) for j, station in stations_df.iterrows()]\n",
    "#        #print(len(tasks))\n",
    "#        #await asyncio.gather(*tasks)\n",
    "#        stations = \",\".join([station.station_code for j, station in stations_df.iterrows()])\n",
    "#        mass_data_downloader(start, end, event_id, stations)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "448b7386",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active 10\n",
      "100 threads took 52.9 seconds\n",
      "100 threads took 54.2 seconds\n",
      "100 threads took 51.2 seconds\n",
      "100 threads took 51.1 seconds\n",
      "100 threads took 52.1 seconds\n",
      "100 threads took 45.9 seconds\n",
      "100 threads took 45.2 seconds\n",
      "100 threads took 47.4 seconds\n",
      "100 threads took 46.3 seconds\n",
      "100 threads took 44.5 seconds\n",
      "100 threads took 44.3 seconds\n",
      "100 threads took 47.0 seconds\n",
      "100 threads took 48.4 seconds\n",
      "100 threads took 45.6 seconds\n",
      "100 threads took 45.0 seconds\n",
      "100 threads took 47.0 seconds\n",
      "100 threads took 47.5 seconds\n",
      "100 threads took 47.6 seconds\n",
      "100 threads took 45.9 seconds\n",
      "100 threads took 45.3 seconds\n",
      "100 threads took 45.0 seconds\n",
      "100 threads took 46.3 seconds\n",
      "100 threads took 45.7 seconds\n",
      "100 threads took 45.1 seconds\n",
      "100 threads took 43.0 seconds\n",
      "100 threads took 42.6 seconds\n",
      "100 threads took 47.2 seconds\n",
      "100 threads took 49.6 seconds\n",
      "100 threads took 45.5 seconds\n",
      "100 threads took 48.4 seconds\n",
      "100 threads took 41.2 seconds\n",
      "100 threads took 43.6 seconds\n",
      "100 threads took 46.5 seconds\n",
      "100 threads took 46.0 seconds\n",
      "100 threads took 44.8 seconds\n",
      "100 threads took 50.3 seconds\n",
      "100 threads took 43.9 seconds\n",
      "100 threads took 47.4 seconds\n",
      "100 threads took 47.0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-06-03 20:18:31,093] - obspy.clients.fdsn.mass_downloader - WARNING: Station information could not be downloaded for NZ.MSZ.10.HHZ. MiniSEED files outside of the station information period will be deleted!\n",
      "[2022-06-03 20:18:31,490] - obspy.clients.fdsn.mass_downloader - WARNING: Station information could not be downloaded for NZ.SYZ.10.HHZ. MiniSEED files outside of the station information period will be deleted!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 threads took 46.9 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-06-03 20:19:26,357] - obspy.clients.fdsn.mass_downloader - WARNING: Station information could not be downloaded for NZ.SYZ.10.HHZ. MiniSEED files outside of the station information period will be deleted!\n",
      "[2022-06-03 20:19:26,359] - obspy.clients.fdsn.mass_downloader - WARNING: Station information could not be downloaded for NZ.MSZ.10.HHZ. MiniSEED files outside of the station information period will be deleted!\n",
      "[2022-06-03 20:19:35,761] - obspy.clients.fdsn.mass_downloader - WARNING: Station information could not be downloaded for NZ.SYZ.10.HHZ. MiniSEED files outside of the station information period will be deleted!\n",
      "[2022-06-03 20:19:43,921] - obspy.clients.fdsn.mass_downloader - WARNING: Station information could not be downloaded for NZ.SYZ.10.HHZ. MiniSEED files outside of the station information period will be deleted!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 threads took 49.3 seconds\n",
      "100 threads took 47.4 seconds\n",
      "100 threads took 45.6 seconds\n",
      "100 threads took 46.1 seconds\n",
      "100 threads took 46.7 seconds\n",
      "100 threads took 45.3 seconds\n",
      "100 threads took 44.0 seconds\n",
      "100 threads took 44.0 seconds\n",
      "100 threads took 46.0 seconds\n",
      "100 threads took 43.3 seconds\n",
      "100 threads took 45.4 seconds\n",
      "100 threads took 43.9 seconds\n",
      "100 threads took 44.4 seconds\n",
      "100 threads took 49.1 seconds\n",
      "100 threads took 45.1 seconds\n",
      "100 threads took 42.3 seconds\n",
      "100 threads took 42.9 seconds\n",
      "100 threads took 46.8 seconds\n",
      "100 threads took 43.5 seconds\n",
      "100 threads took 47.8 seconds\n",
      "100 threads took 42.2 seconds\n",
      "100 threads took 45.0 seconds\n",
      "100 threads took 47.5 seconds\n",
      "100 threads took 45.1 seconds\n",
      "100 threads took 45.9 seconds\n",
      "100 threads took 46.8 seconds\n",
      "100 threads took 44.0 seconds\n",
      "100 threads took 43.3 seconds\n",
      "100 threads took 43.7 seconds\n",
      "100 threads took 49.4 seconds\n",
      "100 threads took 51.6 seconds\n",
      "100 threads took 50.1 seconds\n",
      "100 threads took 47.9 seconds\n",
      "100 threads took 48.3 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-06-03 20:45:18,261] - obspy.clients.fdsn.mass_downloader - WARNING: Station information could not be downloaded for NZ.WKZ.10.HHZ. MiniSEED files outside of the station information period will be deleted!\n",
      "[2022-06-03 20:45:24,381] - obspy.clients.fdsn.mass_downloader - WARNING: Station information could not be downloaded for NZ.WKZ.10.HHZ. MiniSEED files outside of the station information period will be deleted!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 threads took 47.8 seconds\n",
      "100 threads took 45.9 seconds\n",
      "100 threads took 49.3 seconds\n",
      "100 threads took 48.6 seconds\n",
      "100 threads took 45.7 seconds\n",
      "100 threads took 47.2 seconds\n",
      "100 threads took 44.1 seconds\n",
      "100 threads took 44.9 seconds\n",
      "100 threads took 44.4 seconds\n",
      "100 threads took 44.7 seconds\n",
      "100 threads took 47.3 seconds\n",
      "100 threads took 46.2 seconds\n",
      "100 threads took 45.3 seconds\n",
      "100 threads took 49.0 seconds\n",
      "100 threads took 46.9 seconds\n",
      "100 threads took 47.3 seconds\n",
      "100 threads took 46.1 seconds\n",
      "100 threads took 46.8 seconds\n",
      "100 threads took 45.9 seconds\n",
      "100 threads took 50.1 seconds\n",
      "100 threads took 49.0 seconds\n",
      "100 threads took 44.8 seconds\n",
      "100 threads took 47.8 seconds\n",
      "100 threads took 48.7 seconds\n",
      "100 threads took 50.2 seconds\n",
      "100 threads took 47.2 seconds\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "async def final_download_threaded(events):\n",
    "    time1 = time.perf_counter()\n",
    "    tasks = []\n",
    "    #print(\"Initiating mass download request.\")\n",
    "    for i, event in events.iterrows():\n",
    "        event_id = event.event_id\n",
    "        event_time = event['time']\n",
    "        start=event_time - T_event - H_event\n",
    "        end=event_time - H_event\n",
    "        if single: \n",
    "            stations = event['closest_station']\n",
    "        else:\n",
    "            stations = all_station_string\n",
    "        \n",
    "        tasks.append(asyncio.to_thread(mass_data_downloader, start, end, event_id, stations, folder=folder))\n",
    "        \n",
    "    await asyncio.gather(*tasks)\n",
    "    time2 = time.perf_counter()\n",
    "    print(f\"{threads_at_once} threads took {time2-time1:0.1f} seconds\")\n",
    "    \n",
    "#VARIABLES FOR DOWNLOAD\n",
    "\n",
    "T_event = 30\n",
    "if folder == \"normal\": H_event = 1000\n",
    "else: H_event = 10 #10\n",
    "threads_at_once = 100\n",
    "print(folder, H_event)\n",
    "\n",
    "events_filtered = events\n",
    "if station_to_get is not None and single:\n",
    "    events_filtered[events_filtered['closest_station'] == station_to_get]\n",
    "events_filtered = events_filtered[35000:45000] #30000 $23000\n",
    "\n",
    "for event_sublist in [events_filtered[x:x+threads_at_once] for x in range(0, len(events_filtered), threads_at_once)]:\n",
    "    await final_download_threaded(event_sublist)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d3d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pd.interval_range(start=100, end=501, freq=30, closed='right').to_tuples():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "798ad8a5-c196-4527-a8d4-8fdae9132e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210.958631"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pairwise(iterable):\n",
    "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "min([abs(x-y) for x, y in pairwise(events['time'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28febef4-ada6-41a5-8fa0-4a3cd85e5b06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
